{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ejq2GjL5_Rov"
   },
   "source": [
    "Words can be distinguished as *content words* and *stopwords*. Stop words such as articles and prepostiions serve mostly as a grammatical purpose, like filler holding the content words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixys9U9t8YYd"
   },
   "source": [
    "Tokenization\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1585128236906,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "tigBrCXH_3Vt",
    "outputId": "2d468bec-d9d7-44e4-e2f0-a09dd58d3ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "text =\" mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1585128405428,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "c15b0HaSAhLK",
    "outputId": "5d631245-dd33-47e1-990b-fde0a34e5570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'serve', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = u\"Snow White and the Serve Degrees #MakeAMovieCOld@midnight :-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbVcCP8k7jSk"
   },
   "source": [
    "N-Grams\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1585144173375,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "YxFRoDoc8fYu",
    "outputId": "e3dd5a88-83c4-47a1-f7cd-9d6df92a4764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', 'do'], [',', 'do', \"n't\"], ['do', \"n't\", 'slap'], [\"n't\", 'slap', 'the'], ['slap', 'the', 'green'], ['the', 'green', 'witch']]\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
     ]
    }
   ],
   "source": [
    "def n_grams(text,n):\n",
    "  return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned =   ['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n",
    "print(n_grams(cleaned, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEx2qeHB9OO8"
   },
   "source": [
    "Lemmas and Stems\n",
    "================\n",
    "[Stemming and Lemmatization Tutorial](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1585144396930,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "AiVrTrhE9Rwo",
    "outputId": "bb38fad4-9088-4c43-8b99-c2cbc7c0ec71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -->the\n",
      "geese -->goose\n",
      "were -->be\n",
      "waddling -->waddle\n",
      "like -->like\n",
      "mad -->mad\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"The geese were waddling like mad\")\n",
    "for token in doc:\n",
    "  print('{} -->{}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1585145108710,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "rYeHK6Hk_UR1",
    "outputId": "8978f328-9b24-40a2-a4d9-bcf5cb53d5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute -->comput\n",
      "computer -->comput\n",
      "computed -->comput\n",
      "computing -->comput\n",
      "geese -->gees\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = ['compute', 'computer', 'computed', 'computing', 'geese']\n",
    "for token in tokens:\n",
    "    print('{} -->{}'.format(token, stemmer.stem(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQU0hnCVA0BY"
   },
   "source": [
    "Part of Speech Tagging\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1585145173024,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "VOogKp3jA5A7",
    "outputId": "491e4ddc-4052-4e2d-e6f6-345212f58290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -->DET\n",
      "geese -->NOUN\n",
      "were -->AUX\n",
      "waddling -->VERB\n",
      "like -->SCONJ\n",
      "mad -->ADJ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"The geese were waddling like mad\")\n",
    "for token in doc:\n",
    "  print('{} -->{}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4vg2EY2BdxM"
   },
   "source": [
    "Chunking and Named Entity Recognition\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2402,
     "status": "ok",
     "timestamp": 1585145431315,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "9_Aj105rBqBe",
    "outputId": "cbd8209b-d1b5-4317-f4df-9cf3d3157123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary -->NP\n",
      "the green witch -->NP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "  print('{} -->{}'.format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1376,
     "status": "ok",
     "timestamp": 1585145611506,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "5zdFJYcUChuh",
    "outputId": "87a7cd1a-eba2-40ed-d083-f2dd46775752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMShjLkUDj0J"
   },
   "source": [
    "Binary Cross Entropy Loss\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1585207613555,
     "user": {
      "displayName": "Wei Liu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCycLUNcLtkCbSh9RO8f-Cpv11OcGg10a5RYZV=s64",
      "userId": "13873040522997735278"
     },
     "user_tz": -480
    },
    "id": "lP9vbcaQDxJ4",
    "outputId": "b833b40c-d750-48ad-8796-cdcecb1d09e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9000],\n",
      "        [0.0000],\n",
      "        [0.9000],\n",
      "        [0.0000]])\n",
      "tensor(0.0527)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "#probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
    "probabilities = torch.tensor([0.9, 0, 0.9, 0], dtype=torch.float32).view(4,1)\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4,1)\n",
    "loss = bce_loss(probabilities, targets)\n",
    "print(probabilities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2gLhtGG6E_B"
   },
   "source": [
    "Supervised Training Look for a perceptron and binary classification\n",
    "====================================================================\n",
    "\n",
    "\n",
    "```\n",
    "# each epoch is a complete pass over the training data\n",
    "for epoch_i in range(n_epochs):\n",
    "  \n",
    "  # the inner look is over the batches in the dataset\n",
    "  for batch_i in range(n_batches):\n",
    "\n",
    "    # Step 0: Get the data\n",
    "    x_data, y_target = get_toy_data(batch_size)\n",
    "\n",
    "    # Step 1: Clear the gradients\n",
    "    perception.zero_grad()\n",
    "\n",
    "    # Step 2: Compute the forward pass of the model\n",
    "    y_pred = perceptron(x_data, apply_sigmoid=true)\n",
    "\n",
    "    # Step 3: Computer the loss value that we wish to optimise\n",
    "    loss = bce_loss(y_pred, y_target)\n",
    "\n",
    "    # Step 4: Propagate the loss signal backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 5: Trigger the optimizer to perform one update\n",
    "    optimizer.step()\n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSnfNyr5FnhR8RKG0ySUfP",
   "collapsed_sections": [],
   "name": "Week12-Notes.ipynb",
   "provenance": [
    {
     "file_id": "1MqJRZoCqPgraWAE_JKZmP7I67f_Ff1ZE",
     "timestamp": 1585193013459
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
