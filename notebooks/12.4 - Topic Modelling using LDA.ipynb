{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6UXR_M4Dt3b"
   },
   "source": [
    "# Week 12.3 - Topic Modelling using LDA\n",
    "\n",
    "## Preparation\n",
    "\n",
    "In this exercise, we will need to make sure the following packages are installed into your conda environment that runs this Jupyter notebook. \n",
    "\n",
    "`pip install gensim spacy pyLDAvis matplotlib numpy pandas nltk`\n",
    "\n",
    "Note: This notebook takes approximately 50 minutes to execute all cells. The visualisations of the topic models is the culprit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgNh7kvdDt3k"
   },
   "outputs": [],
   "source": [
    "# For Google Drive mounting\n",
    "# from google.colab import drive\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRIvMTkKDt3q"
   },
   "source": [
    "### Define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1730,
     "status": "ok",
     "timestamp": 1585055263059,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "3NJiXJeIEG2V",
    "outputId": "bdedd074-d0c3-4225-9eb8-a9a5676a4f0b"
   },
   "outputs": [],
   "source": [
    "# Need to download stopwords before they can be used\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1585055263060,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "PProfHduDt3r",
    "outputId": "1632d2d4-e290-466e-e31c-39d51ff45d58"
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "pprint(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnlbAvI1Dt3v"
   },
   "source": [
    "### Import the GSWA dataset\n",
    "\n",
    "The 20-Newsgroups dataset contains about 6k data posts from 1,135 GSWA reports. \n",
    "\n",
    "The dataset can be imported using `pandas.read_json` and we can see the dataset has 3 columns. The `target-name` is the category or topics that the news is manually assigned to. \n",
    "\n",
    "We choose to use this dataset because it has pre-assigned categories according to topics, so our clustering results can be readily compared. In real world data, this is not always easily available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3100,
     "status": "ok",
     "timestamp": 1585055269331,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "LxhLbOgZDt3w",
    "outputId": "ec1eab80-3958-4175-c77f-401a2bd1d81f"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "# Import Dataset\n",
    "\n",
    "data_path = r\"../data/wamex_xml.zip\"\n",
    "\n",
    "data = list()\n",
    "with zipfile.ZipFile(data_path, \"r\") as z:\n",
    "    #df = [pd.read_json(filename) for filename in z.namelist()]\n",
    "    print(len(z.namelist()))\n",
    "    for filename in z.namelist():\n",
    "        # print(filename)\n",
    "        # df = pd.read_json(filename)\n",
    "        with z.open(filename) as f:\n",
    "            # load the json file\n",
    "            # The resulting `content` is a list\n",
    "            content = json.loads(f.read()) \n",
    "            # Convert content to a string   \n",
    "            content = \"\".join(content)\n",
    "            # Add to the data list\n",
    "            data.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1585055269332,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "tbH1YzboEla8",
    "outputId": "5a26846d-d56d-4518-a827-6153eaac9ba5"
   },
   "outputs": [],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCpmfWmvDt3z"
   },
   "source": [
    "## Pre-processing\n",
    "### Removing unnecessary information\n",
    "\n",
    "We can use regular expressions for this purpose by untilising the `re` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1585055273096,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "T-OOwCw3Dt30",
    "outputId": "041cea8c-f738-4371-8613-69f9796099db"
   },
   "outputs": [],
   "source": [
    "# Remove Figure No. \n",
    "data = [re.sub('Figure\\s+\\d+', '', sent) for sent in data]\n",
    "\n",
    "# Remove all numbers\n",
    "data = [re.sub('\\d+', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7MUDv80Dt36"
   },
   "source": [
    "### Tokenisation\n",
    "\n",
    "We need to break down each sentence into a list of words, for the data to be ready for consumption by `gensim`. The `simple_preprocess()` function is built for such purposes. We can also set the `deacc=True` to remove punctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9REbhKiE0rl"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7899,
     "status": "ok",
     "timestamp": 1585055280854,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "MeF4d8J-Dt37",
    "outputId": "5ed89355-39c8-4d4a-b80a-1cbc7d33a7ed"
   },
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "pprint(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwrikYaRDt3-"
   },
   "source": [
    "### Creating bigram and trigrams - Gensim’s Phrases model\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25774,
     "status": "ok",
     "timestamp": 1585055301850,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "eQ75jfFIDt4A",
    "outputId": "49dc8044-819d-4a30-84ea-984d2a194bd1"
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3T09XJCTE6tc"
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37800,
     "status": "ok",
     "timestamp": 1585055314439,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ZOG0O89bDt4D",
    "outputId": "1e42ba86-a15a-490d-e7bc-c7071058cabb"
   },
   "outputs": [],
   "source": [
    "# See trigram example\n",
    "pprint(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Okh_VY-GDt4H"
   },
   "source": [
    "### Build functions for the pre-processing pipeline\n",
    "\n",
    "Remove stopwords, make bigram and trigrams, and lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7N50O8dDt4I"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gb9hsNz_Dt4M"
   },
   "source": [
    "### Call the functions in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZcVI_ZVDt4N"
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-I40DYqDt4Q"
   },
   "source": [
    "#### Caution on the lemmatiser\n",
    "\n",
    "During the processing of the data, we realised that some documents is empty. We also know that the lemmatiser has a restriction on processing text no longer than 1000000 characters. So let's filter those documents out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46697,
     "status": "ok",
     "timestamp": 1585055327544,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "S3qlr74MDt4R",
    "outputId": "6bb761d9-0361-40b8-88a7-d65b3adf05ac"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(len(data_words_bigrams))\n",
    "\n",
    "for text in data_words_bigrams:\n",
    "    doc = \" \".join(text)\n",
    "    #print(len(doc))\n",
    "    if(len(doc)==0 or len(doc)>1000000):\n",
    "        data_words_bigrams.pop(index)\n",
    "        \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46509,
     "status": "ok",
     "timestamp": 1585055327545,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "_L-SlOW3FHDN",
    "outputId": "f767b20f-604d-4f85-8fbb-cf9b9ef6ba47"
   },
   "outputs": [],
   "source": [
    "print(len(data_words_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52330,
     "status": "ok",
     "timestamp": 1585055333504,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Jl4W2FT-FK_8",
    "outputId": "d31f89ea-cccb-4ebe-eb20-a7ff91357ed9"
   },
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# !python -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110466,
     "status": "ok",
     "timestamp": 1585055393534,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "csjbAqY5Dt4U",
    "outputId": "894adfef-9ee5-474e-c855-92debb9bfd56"
   },
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baZWBbnBDt4Z"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 111407,
     "status": "ok",
     "timestamp": 1585055394784,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "v0icteGfFXEp",
    "outputId": "e47c4732-e9bd-4da2-a7cf-6416f87af4f1"
   },
   "outputs": [],
   "source": [
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 111271,
     "status": "ok",
     "timestamp": 1585055394785,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Eumsz9s_Dt4d",
    "outputId": "8f5ddcdf-4921-4bdb-b47b-015302e0752d"
   },
   "outputs": [],
   "source": [
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109507,
     "status": "ok",
     "timestamp": 1585055394785,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "EJn-e0xxDt4i",
    "outputId": "77dd84f3-15dd-4628-9286-b89d8d490146"
   },
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109350,
     "status": "ok",
     "timestamp": 1585055394786,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "8o9K-J3LDt4l",
    "outputId": "aeecbccb-e2e5-4d1c-a171-ba68ee5b0a5e"
   },
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109206,
     "status": "ok",
     "timestamp": 1585055394786,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "e8_6nEnGDt4o",
    "outputId": "df1f24f1-07ba-40b3-c901-92812b0a420a"
   },
   "outputs": [],
   "source": [
    "print(id2word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109065,
     "status": "ok",
     "timestamp": 1585055394786,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "stObVclRDt4r",
    "outputId": "c8743af7-5a37-4580-e1be-a321cf804d08"
   },
   "outputs": [],
   "source": [
    "print(corpus[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107306,
     "status": "ok",
     "timestamp": 1585055394787,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "mAOCYcq0Dt4w",
    "outputId": "96c419d2-bb4c-4b5f-bb23-89f7b7f83fd5"
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(corpus[0], key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_by_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107183,
     "status": "ok",
     "timestamp": 1585055394787,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "HRuNQmZKDt4z",
    "outputId": "43e5bccd-3ef5-4580-8847-d0bb32bc1845"
   },
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[(id2word[id], freq) for id, freq in sorted_by_value] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJB50q8AFeE6"
   },
   "source": [
    "## Train LDA Model\n",
    "Number of topics is set to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgj-Ob96Dt43"
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 125897,
     "status": "ok",
     "timestamp": 1585055416270,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "zJfngkYiDt46",
    "outputId": "e14fe77c-70a6-4883-a9f6-26f31a9aff96"
   },
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 173002,
     "status": "ok",
     "timestamp": 1585055464987,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "1-dKSB0iDt49",
    "outputId": "db882fd4-1589-4e22-a8bc-44f005a6c5e5"
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSGCyT8WDt5B"
   },
   "source": [
    "### Visualising LDA \n",
    "\n",
    "#### Wait, what am I looking at again?\n",
    "\n",
    "There are a lot of moving parts in the visualization. Here's a brief summary:\n",
    "\n",
    "On the left, there is a plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map)\n",
    "The plot is rendered in two dimensions according a multidimensional scaling (MDS) algorithm. Topics that are generally similar should be appear close together on the plot, while dissimilar topics should appear far apart.\n",
    "\n",
    "The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "On the right, there is a bar chart showing top terms.\n",
    "When no topic is selected in the plot on the left, the bar chart shows the top most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "When a particular topic is selected, the bar chart changes to show the top most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter λλ, which can be adjusted with a slider above the bar chart.\n",
    "\n",
    "Setting the λλ parameter close to 1.0 (the default) will rank the terms solely according to their probability within the topic.\n",
    "\n",
    "Setting λλ close to 0.0 will rank the terms solely according to their \"distinctiveness\" or \"exclusivity\" within the topic — i.e., terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "Setting λλ to values between 0.0 and 1.0 will result in an intermediate ranking, weighting term probability and exclusivity accordingly.\n",
    "\n",
    "Rolling the mouse over a term in the bar chart on the right will cause the topic circles to resize in the plot on the left, to show the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "A more detailed explanation of the pyLDAvis visualization can be found here. Unfortunately, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics. If you need to match up topics in gensim's LdaMulticore object and pyLDAvis' visualization, you have to dig through the terms manually.\n",
    "\n",
    "#### Analyzing our LDA model\n",
    "The interactive visualization pyLDAvis produces is helpful for both:\n",
    "\n",
    "Better understanding and interpreting individual topics, and Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most freqeuent and/or \"relevant\" terms, using different values of the λλ parameter. This can help when you're trying to assign a human interpretable name or \"meaning\" to each topic.\n",
    "\n",
    "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "#### Describing text with LDA\n",
    "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% Topic A, 20% Topic B, 20% Topic C, and 10% Topic D.\n",
    "\n",
    "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n",
    "\n",
    "Using spaCy to remove punctuation and lemmatize the text\n",
    "\n",
    "- Applying our first-order phrase model to join word pairs\n",
    "- Applying our second-order phrase model to join longer phrases\n",
    "- Removing stopwords\n",
    "- Creating a bag-of-words representation\n",
    "- Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The lda_description(...) function will perform all these steps for us, including printing the resulting topical description of the input text.\n",
    "\n",
    "##### Acknowledgement: https://www.kaggle.com/navinch/interesting-visualizations-lda-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1213516,
     "status": "ok",
     "timestamp": 1585056508049,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "KwprcYdADt5C",
    "outputId": "4243c9a6-27b6-4901-bb2d-8aff5560dde4"
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WZHH9tDDt5R"
   },
   "source": [
    "## How to find the best number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5w1LvRPDt5T"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2915761,
     "status": "ok",
     "timestamp": 1585058220648,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "0yOYit5xDt5W",
    "outputId": "5bf4c2da-7312-4112-e62e-70cdedca1538"
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2912841,
     "status": "ok",
     "timestamp": 1585058220652,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "mRe4-DSTDt5Z",
    "outputId": "316750ce-341b-4903-d631-fb8b63a3138b"
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "12.3-LDA-GSWA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
