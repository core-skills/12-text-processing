{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEsV0AtG7JQd"
   },
   "source": [
    "# Word vector visualization with Gensim\n",
    "\n",
    "Source: üòä[Day 12 - Special Data Types: Natural Language Processing](https://github.com/core-skills/12-text-processing) *repository*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚òùÔ∏èBefore moving on with this notebook, ensure that you have:\n",
    "- Downloaded the *glove.6B.100d.txt* embeddings and placed them in the `./data` directory. If not, [download](http://nlp.stanford.edu/data/glove.6B.zip) and save them before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**:\n",
    "In this notebook we will explore word vectors. To achieve this we will use the [Gensim](https://radimrehurek.com/gensim/) library with pretrained [GloVe vectors](https://nlp.stanford.edu/projects/glove/). Gensim allows us to convert a file of GloVe vectors into word2vec format. The 100d GloVe embeddings are used within the notebook, however there are various dimensions such as 50 and 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png\" alt=\"\" style=\"width:800px;\"/>\n",
    "\n",
    "[analyticsvidhya](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplementary Content**: Check out https://rare-technologies.com/word2vec-tutorial/ for an interactive web-based application that allows you to do play with different functionalties of word embeddings.\n",
    "\n",
    "Adapted from: *CS224n: Natural Language Processing with Deep Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Word Vectors](#word_vectors)\n",
    "2. [Word Similarities](#word_similarities)\n",
    "3. [Word Analogies](#word_analogies)\n",
    "4. [Visualising Word Vectors](#vector_visualisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "- [numpy](https://numpy.org/) - library that we will use for helping visualise word vectors\n",
    "- [matplotlib](https://matplotlib.org/) - library that we will use for plotting the data\n",
    "- [gensim](https://radimrehurek.com/gensim/) - library that we will use to experiment with word embeddings/vectors\n",
    "- [sklearn](https://scikit-learn.org/) - library that we will use for performing dimensionality reduction to help visualise word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the notebook environment and load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes printing lists prettier \n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGLymbOH7JQk"
   },
   "outputs": [],
   "source": [
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_similarities(similarities: List[tuple]) -> List[str]:\n",
    "    ''' Prettifies list of word similarities produced by Gensim.\n",
    "    '''\n",
    "    longest_str = max([len(sim[0]) for sim in similarities])\n",
    "    return \"\\n\".join([f'{idx+1}.\\t{sim[0]:{longest_str+1}}\\t{sim[1]*100:0.1f}%' for idx, sim in enumerate(similarities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rY4HptWCPf8"
   },
   "source": [
    "### Load pretrained word embeddings from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/glove.6B.50d.txt').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43498,
     "status": "ok",
     "timestamp": 1585112775231,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "hO_RpzPk7JQu",
    "outputId": "486cc377-d912-4c91-bfba-cc9fb48863c9"
   },
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "glove_file = datapath(data_path)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.50d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors <a name=\"word_vectors\"></a>\n",
    "Now that we have loaded the pre-trained word embedding model, let's unpack it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'france'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical representation words\n",
    "# Note: if we add words that are out-of-vocabulary (e.g. not in the corpus the model was trained on, we'll receive an error)\n",
    "model[test_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the shape of the embeddings that are produced for a given word\n",
    "model[test_word].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeI-Msu3CWzj"
   },
   "source": [
    "## Word Similarities<a name=\"word_similarities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pre-trained word vectors, we can perform simple distance operations such as finding similar words e.g. finding those that are the closest in vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prettify_similarities(model.most_similar('obama')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43484,
     "status": "ok",
     "timestamp": 1585112775233,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "PjRgmVyPnGjX",
    "outputId": "b715bf29-7f6c-42cf-ac63-c15eb0133a55"
   },
   "outputs": [],
   "source": [
    "print(prettify_similarities(model.most_similar('gold')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43480,
     "status": "ok",
     "timestamp": 1585112775234,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "HrxGXW-67JQ8",
    "outputId": "a5993d25-b807-4137-ad78-3325f67acefb"
   },
   "outputs": [],
   "source": [
    "print(prettify_similarities(model.most_similar('apple')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43475,
     "status": "ok",
     "timestamp": 1585112775234,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "zuFH2u-G7JRF",
    "outputId": "b979a409-1a10-448e-ad8a-909b2964d973"
   },
   "outputs": [],
   "source": [
    "print(prettify_similarities(model.most_similar(negative='apple')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies - \"A is to B as C is to?\"<a name=\"word_analogies\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: *king - man + woman = queen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlwhiz.com/images/word2vec.png\" alt=\"Word analogy example\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43470,
     "status": "ok",
     "timestamp": 1585112775235,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "PMQG0KFr7JRM",
    "outputId": "116fc2dd-9202-4012-e467-3ec825a91854"
   },
   "outputs": [],
   "source": [
    "print(prettify_similarities(model.most_similar(positive=['woman', 'king'], negative=['man'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mJkLtBJ7JRU"
   },
   "outputs": [],
   "source": [
    "def analogy(x1: str, x2: str, y1: str):\n",
    "    '''Finds missing word form partial analogy'''\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    print(f'{x1} is to {x2} as {y1} is to \\033[1m{result[0][0]}\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43458,
     "status": "ok",
     "timestamp": 1585112775236,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ELaA5Jwi7JRf",
    "outputId": "dd6c0180-32c0-4745-9193-79a5659ff101"
   },
   "outputs": [],
   "source": [
    "analogy('japan', 'japanese', 'australia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43947,
     "status": "ok",
     "timestamp": 1585112775731,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "BQ68h3jE7JRk",
    "outputId": "256f107e-75e5-4c19-aa76-f55e649cc95b"
   },
   "outputs": [],
   "source": [
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43943,
     "status": "ok",
     "timestamp": 1585112775732,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "c2YhPCBj7JRp",
    "outputId": "3788c460-7bf0-45ed-c20b-383d7b6b6085"
   },
   "outputs": [],
   "source": [
    "analogy('obama', 'clinton', 'reagan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43938,
     "status": "ok",
     "timestamp": 1585112775732,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "lZo4SXIX7JRt",
    "outputId": "a07204a7-9950-4627-d4a0-9e6a686d0f02"
   },
   "outputs": [],
   "source": [
    "analogy('tall', 'tallest', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43932,
     "status": "ok",
     "timestamp": 1585112775732,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "W6HYFWVn7JRy",
    "outputId": "797f84c8-ddd0-4a80-d340-65f544b52cfe"
   },
   "outputs": [],
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy('gold', 'copper', 'oil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKdJVbJrEMfH"
   },
   "source": [
    "## Find the odd word out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_odd_one_out(words: List[str]) -> str:\n",
    "    '''Finds odd word out of list of words'''\n",
    "    assert type(words) is list\n",
    "    odd_one = model.doesnt_match(words)\n",
    "    words_marked = \" \".join([word if word != odd_one else f'\\033[1m{word}\\033[0m' for word in words])\n",
    "    print(words_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43928,
     "status": "ok",
     "timestamp": 1585112775733,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ye1_2nPJ7JR4",
    "outputId": "ba008d81-adb7-4586-90bd-a28ccedb860a"
   },
   "outputs": [],
   "source": [
    "find_odd_one_out([\"breakfast\", \"scereal\", \"dinner\", \"lunch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_odd_one_out([\"copper\",\"gold\",\"iron\",\"oil\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKEUO-gO7JR8"
   },
   "source": [
    "## Visualising Word Vectors<a name=\"vector_visualisation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kd2uLi-y7JSB"
   },
   "outputs": [],
   "source": [
    "def generate_visualisation(model, words: List[str]=None, sample_size: int=0):\n",
    "    '''Displays scatterplot of dimensionality reduced word vectors using principal component analysis (PCA)\n",
    "    \n",
    "    Note:\n",
    "        - If no words are provided, a random set will be sampled from the embedding models vocabulary.\n",
    "    '''\n",
    "\n",
    "    if words == None:\n",
    "        if sample_size > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample_size)\n",
    "        else:\n",
    "            words = [word for word in model.vocab]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='b', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise groups of beverages, foods, animals, locations, etc, to see how they cluster in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44728,
     "status": "ok",
     "timestamp": 1585112776543,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "3_7_va6r7JSI",
    "outputId": "4aebfffd-8c8f-4cd6-84fb-107a20dc7d78"
   },
   "outputs": [],
   "source": [
    "generate_visualisation(model,\n",
    "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
    "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "                         'school', 'college', 'university', 'institute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise randomly sampled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45941,
     "status": "ok",
     "timestamp": 1585112777761,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "DZJsi7od7JSL",
    "outputId": "463e173d-5ba2-4784-a2a6-e91fd19f0530"
   },
   "outputs": [],
   "source": [
    "generate_visualisation(model, sample_size=50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "12.2.1-Gensim word vector visualization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
