{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyRjwrQynR22"
   },
   "source": [
    "# Learning Word Embeddings\n",
    "\n",
    "Source: üòä[Day 12 - Special Data Types: Natural Language Processing](https://github.com/core-skills/12-text-processing) *repository*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚òùÔ∏èBefore moving on with this notebook, ensure that you have:\n",
    "- Downloaded the [hotels review data](https://github.com/kavgan/nlp-in-practice/blob/master/word2vec/reviews_data.txt.gz) (*reviews_data.txt.gz*) and placed it in the `./data` directory.\n",
    "- Downloaded the GSWA data (*wamex_xml.zip*) and placed it in the `./data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**: Generating word embeddings using Gensim Word2Vec... Word Embeddings and Word2Vec\n",
    "In this notebook, rather than loading word embeddings trained on large generic datasets, we train our own embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfoKOGconR29"
   },
   "source": [
    "This notebook uses two types of datasets.\n",
    "1. General Domain: 259,000 *hotel reviews* from [OpinRank](https://archive.ics.uci.edu/ml/datasets/opinrank+review+dataset) \n",
    "2. Domain Specific: *geological surveys* (GSWA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplementary Content**: ...s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: [*Kavita Ganesan's tutorial*](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/) ([*repository*](https://github.com/kavgan/data-science-tutorials/tree/master/word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Understanding Word2Vec parameters](#word2vec_parameters)\n",
    "2. [Building general-domain word vectors](#general-domain_word_vectors)\n",
    "3. [Building domain-specific word vectors](#domain-specific_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "- [logging](https://docs.python.org/3/library/logging.html) - library that we use to log and track progress of data preprocessing\n",
    "- [gzip](https://docs.python.org/3/library/gzip.html) - library that we use to read data in .zip format\n",
    "- [tqdm](https://github.com/tqdm/tqdm) - library that we use to track the progress of various intensive operations\n",
    "- [bokeh](https://bokeh.org/) - library that we use to interactively visualise word vectors \n",
    "- [gensim](https://radimrehurek.com/gensim/) - library that we will use to experiment with word embeddings/vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PliE9yXYnR25"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the notebook environment and load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file_path: str) -> List:\n",
    "    '''Parses input file which is in gzip format'''\n",
    "    assert input_file_path.endswith('.gz')\n",
    "    \n",
    "    corpus = []\n",
    "    with gzip.open(input_file_path, 'rb') as f:\n",
    "        for line in tqdm(f, desc=\"Reading file\"): \n",
    "            # Perform pre-processing and return a list of words from each review text \n",
    "            corpus.append(gensim.utils.simple_preprocess(line))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_similarities(similarities: List[tuple]) -> List[str]:\n",
    "    ''' Prettifies list of word similarities produced by Gensim.\n",
    "    '''\n",
    "    longest_str = max([len(sim[0]) for sim in similarities])\n",
    "    return \"\\n\".join([f'{idx+1}.\\t{sim[0]:{longest_str+1}}\\t{sim[1]*100:0.1f}%' for idx, sim in enumerate(similarities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5YUtIH_nR3J"
   },
   "source": [
    "## Understanding Word2Vec Parameters<a name=\"word2vec_parameters\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our custom embedding models, we need to understand some of the models parameters. For reference, this is the command that we will use to train the model: `model = gensim.models.Word2Vec(sentences=documents, size=150, window=10, min_count=2, workers=10)`\n",
    "\n",
    "#### Parameters of Interest\n",
    "`sentences`: The corpus that the model will be trained on in the format of a list of lists of tokens.\n",
    "\n",
    "`size`: The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. Typical sizes are 100-300. A value of 100-150 has worked well for me. \n",
    "\n",
    "`window`: The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.\n",
    "\n",
    "`min_count`: Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "`workers`: How many threads to use behind the scenes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè¢ Learning General-Domain Word Embeddings <a name=\"general-domain_word_vectors\"></a>\n",
    "In this section we will learn word embeddings from a general-domain (hotel reviews). The hotel reviews contains:\n",
    "- Full reviews of hotels in 10 different cities (Dubai, Beijing, London, New York City, New Delhi, San Francisco, Shanghai, Montreal, Las Vegas, Chicago) \n",
    "- There are about 80-700 hotels in each city \n",
    "- Extracted fields include date, review title and the full review \n",
    "- Total number of reviews: ~259,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process hotel reviews dataset\n",
    "Before learning word embeddings, we need to load and pre-process the hotel reviews corpus. The helper function `read_input` aids us with this task. This helper parses the dataset that is in .zip format and composed of numerous .json data files. These are read into memory and pre-processed with Gensim's `gensim.utils.simple_preprocess()` function that converts each review into a list of tokens that are lower cased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏èLoading the hotel reviews documents will take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading file: 255404it [01:07, 3799.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# The data_file path will be different depending on where you've copied the notebooks...\n",
    "data_file = '../data/reviews_data.txt.gz'\n",
    "\n",
    "# Read the tokenized reviews into a list each review item becomes a series of words so this becomes a list of lists\n",
    "documents = read_input(input_file_path=data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oct nice trendy hotel location not too bad stayed in this hotel for one night as this is fairly new place some of the taxi drivers did not know where it was and or did not want to drive there once have eventually arrived at the hotel was very pleasantly surprised with the decor of the lobby ground floor area it was very stylish and modern found the reception staff geeting me with aloha bit out of place but guess they are briefed to say that to keep up the coroporate image as have starwood preferred guest member was given small gift upon check in it was only couple of fridge magnets in gift box but nevertheless nice gesture my room was nice and roomy there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by bliss the location is not great it is at the last metro stop and you then need to take taxi but if you are not planning on going to see the historic sites in beijing then you will be ok chose to have some breakfast in the hotel which was really tasty and there was good selection of dishes there are couple of computers to use in the communal area as well as pool table there is also small swimming pool and gym area would definitely stay in this hotel again but only if did not plan to travel to central beijing as it can take long time the location is ok if you plan to do lot of shopping as there is big shopping centre just few minutes away from the hotel and there are plenty of eating options around including restaurants that serve dog meat\n"
     ]
    }
   ],
   "source": [
    "# reviewing the tokenized hotel reviews documents\n",
    "print(\" \".join(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training general-domain word embeddings\n",
    "Here we set-up our Word2Vec embedding model and train it on the documents in the hotel reviews dataset. After training, we save it so it can be used without requiring re-training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Training the embedding model will take a while due to the 250k documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set-up model\n",
    "model = gensim.models.Word2Vec(sentences=documents, size=150, window=10, min_count=2, workers=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train model on hotel reviews corpus\n",
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save trained embedding models\n",
    "model.save(\"../data/word2vec/word2vec_reviews.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì£ Alternatively, load the embeddings that we've already pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = Path('../data/word2vec/word2vec_reviews.bin').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(str(modelpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing learnt word embeddings\n",
    "Similar to the notebook [12.2.1 - Word vector visualisation with Gensim](), we will explore the similarity of words represented with our general-domain embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettify_similarities(model.wv.most_similar(['polite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301597,
     "status": "ok",
     "timestamp": 1585117062355,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "0m_4auJdvOea",
    "outputId": "4985a791-2cb3-4318-ad1d-653284363f92"
   },
   "outputs": [],
   "source": [
    "# Get vector representation of a word\n",
    "testWord = 'dirty'\n",
    "print(f'Word - {testWord}\\nVector Representation\\n{model.wv.get_vector(testWord)}')\n",
    "print(f'Shape of vector: {model.wv.get_vector(testWord).shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301911,
     "status": "ok",
     "timestamp": 1585117062681,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "mT-2WDWsnR3R",
    "outputId": "f0e2140f-2666-4ead-9382-903666f12b56"
   },
   "outputs": [],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301899,
     "status": "ok",
     "timestamp": 1585117062682,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "bE4bVMGfnR3Y",
    "outputId": "4035ea5b-9d7e-43d4-f38c-fb653f986c01"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301888,
     "status": "ok",
     "timestamp": 1585117062683,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "VOq0zzvGnR3c",
    "outputId": "d3384fbb-8d85-4472-b329-fe10b2b1be70"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301878,
     "status": "ok",
     "timestamp": 1585117062684,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "LmAkHmOqnR3f",
    "outputId": "5ab1e920-b9ad-49dd-d134-9c6110f88540"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301826,
     "status": "ok",
     "timestamp": 1585117062684,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Z9-X6bgYnR3l",
    "outputId": "37054ef7-07c7-4087-aa10-a45c53fc95ca"
   },
   "outputs": [],
   "source": [
    "# Get everything related to stuff on the bed\n",
    "w1 = ['bed','sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar(positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301816,
     "status": "ok",
     "timestamp": 1585117062685,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Hy6BPFqBnR3p",
    "outputId": "a81165a7-c9c9-4015-fc8f-c5fc6c9c786a"
   },
   "outputs": [],
   "source": [
    "# Similarity between two different words\n",
    "print(f'Similarity between the words dirty and smelly: {model.wv.similarity(w1=\"dirty\",w2=\"smelly\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301796,
     "status": "ok",
     "timestamp": 1585117062685,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "rGb75GUznR3t",
    "outputId": "03339e12-a10b-4f58-f66c-1e3e07ca4f26"
   },
   "outputs": [],
   "source": [
    "# Similarity between two identical words\n",
    "print(f'Similarity between the same word dirty: {model.wv.similarity(w1=\"dirty\",w2=\"dirty\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301780,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "CavnmhyKnR3w",
    "outputId": "c635b945-00bc-4bee-8843-1a83ee64593e"
   },
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "print(f'Similarity between the words dirty and clean: {model.wv.similarity(w1=\"dirty\",w2=\"clean\"):0.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301769,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "3PAyBhoEnR3z",
    "outputId": "6da594e8-ee85-4a08-ae28-772317c1f044"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "oddOneOutList = [\"cat\", \"dog\", \"france\"]\n",
    "print(f'Which words doesnt belong in the set [cat, dog, france]? Odd one out: {model.wv.doesnt_match(oddOneOutList)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301758,
     "status": "ok",
     "timestamp": 1585117062686,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "veE4VDR_nR33",
    "outputId": "9ba6787f-0c32-4f72-f18a-1fef0789c9a2"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "oddOneOutList = [\"bed\",\"pillow\",\"duvet\",\"shower\"]\n",
    "print(f'Which words doesnt belong in the set [bed, pillow, duvet, shower]? Odd one out: {model.wv.doesnt_match(oddOneOutList)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ0FBQUrnR38"
   },
   "source": [
    "## Visualising the word vectors in 2D space\n",
    "\n",
    "Here we use a dimensionality reduction and visualisation package from Scikit-Learn, t-Distributed Stochastic Neighbor Embedding (t-SNE), which is particularly suited for visualising high dimensional data. Another popular options for dimensionality reduction is Principal Component Analysis (PCA).\n",
    "\n",
    "Note: the dimension of our word vectors by choice is 150, typical numbers could be 50, 100, and 300. Among these numbers, a dimension of 300 has been shown as most effective in capturing the syntatic and semantic information of a word. However, it will take much longer to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3yKAcTTnR39"
   },
   "outputs": [],
   "source": [
    "# Load required statistical package\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# Need the interactive Tools for Matplotlib\n",
    "%matplotlib inline\n",
    "# Plot formatting\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "font = {'size':16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NQbN-AynR4B"
   },
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,150), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.title(f'Words closest to: {word}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIHec5Phx4Ll"
   },
   "source": [
    "### Visualising closest words in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1302471,
     "status": "ok",
     "timestamp": 1585117064024,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "YyKBFzzVnR4E",
    "outputId": "77fc97fe-e27b-4509-99f8-849cbd6af4b3"
   },
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model.wv, \"bed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9nDZAdKy_B1"
   },
   "source": [
    "## ‚öôÔ∏è Learning Domain-Specific Word Embeddings <a name=\"domain-specific_word_vectors\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are interesting and thought provoking but\n",
    "- What insights can we gain from them?\n",
    "- How do we apply them to industry data?\n",
    "\n",
    "This section of the notebook will explore word embeddings and their application to geological survey data (GSWA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5MWs23uzPUP"
   },
   "source": [
    "## Loading a small sample of Geological Survey of Western Australia (GSWA) data\n",
    "Note: we have already added our Google Drive to the notebook, so we don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9yX-kZ10crm"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cn-cuSk40cjo"
   },
   "outputs": [],
   "source": [
    "data_file=\"../data/wamex_xml.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1585118724590,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "S2zNT1qWzPMt",
    "outputId": "26a9d507-1663-4e9e-d59f-7cecd158a83c"
   },
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "data = list()\n",
    "with zipfile.ZipFile(data_file, \"r\") as z:\n",
    "    #df = [pd.read_json(filename) for filename in z.namelist()]\n",
    "    print(len(z.namelist()))\n",
    "    for filename in z.namelist():\n",
    "        # print(filename)\n",
    "        # df = pd.read_json(filename)\n",
    "        with z.open(filename) as f:\n",
    "            # load the json file\n",
    "            # The resulting `content` is a list\n",
    "            content = json.loads(f.read()) \n",
    "            # Convert content to a string   \n",
    "            content = \"\".join(content)\n",
    "            # Add to the data list\n",
    "            data.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2399,
     "status": "ok",
     "timestamp": 1585118724592,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Cph4A7YQzPKA",
    "outputId": "22330ef8-001b-4a61-b4a8-7df00695e3e8"
   },
   "outputs": [],
   "source": [
    "# Previewing the data that we have loaded. This is very different to the hotel reviews dataset.\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RE0Yjy_gzPHe"
   },
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in zip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "\n",
    "    data = list()\n",
    "    with zipfile.ZipFile(input_file, \"r\") as z:\n",
    "    #df = [pd.read_json(filename) for filename in z.namelist()]\n",
    "        print(len(z.namelist()))\n",
    "        for i, filename in enumerate(z.namelist()):\n",
    "            # print(filename)\n",
    "            # df = pd.read_json(filename)\n",
    "            if (i%100==0):\n",
    "                logging.info (\"read {0} reports\".format (i))\n",
    "            with z.open(filename) as f:\n",
    "                # load the json file\n",
    "                # The resulting `content` is a list\n",
    "                content = json.loads(f.read()) \n",
    "                # Convert content to a string   \n",
    "                content = \"\".join(content)\n",
    "                if len(content) >= 10:\n",
    "                    # Add to the data list\n",
    "                    yield gensim.utils.simple_preprocess (content)\n",
    "                else:\n",
    "                    logging.info(\"removed {0} because of small size\".format (filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9455,
     "status": "ok",
     "timestamp": 1585118732100,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "IIgxeD4XzPEn",
    "outputId": "f3380ec9-c5f4-456c-d7f0-c8d4b3a8bec2"
   },
   "outputs": [],
   "source": [
    "# Read the tokenized reviews into a list each review item becomes a series of words so this becomes a list of lists\n",
    "documents = list(read_input (data_file))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9324,
     "status": "ok",
     "timestamp": 1585118732101,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ok-NJAsoyBEr",
    "outputId": "1a98cad1-cc07-439b-8303-57750559aa1d"
   },
   "outputs": [],
   "source": [
    "# Review the first documents top 25 words. See how they have been pre-processed and tokenized.\n",
    "print(documents[0][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOHp-kJ01Tt2"
   },
   "source": [
    "### Training word embedding model off of domain-specific text.\n",
    "\n",
    "Note: we are required to use bigrams to aid our model due to domain-specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-qVxg6foJMN"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183770,
     "status": "ok",
     "timestamp": 1585118907018,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "OjXRu31I0-HS",
    "outputId": "50d20d02-f3c5-40cc-e05d-d3c0f121b05a"
   },
   "source": [
    "# Build phrases and bigrams to train our word2vec model on\n",
    "phrases = gensim.models.Phrases(documents, min_count=1, threshold=1)\n",
    "bigrams = gensim.models.phrases.Phraser(phrases)\n",
    "\n",
    "# Note: we're referring to the model as DS (domain-specific). This allows us\n",
    "# to make a contrast between the pretrained word embeddings and the domain-specific ones. \n",
    "modelDS = gensim.models.Word2Vec (bigrams[documents], size=50, window=10, min_count=2, workers=12)\n",
    "modelDS.train(bigrams[documents],total_examples=len(documents), epochs=10)\n",
    "modelDS.save(\"../data/word2vec/word2vec_gswa.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the hotel reviews, we load pre-trained embeddings rather than training them due to time requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDS = gensim.models.Word2Vec.load('../data/word2vec/word2vec_gswa.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgB_ATFn1eFd"
   },
   "source": [
    "## Looking at domain-specific outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183387,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "Ocw0iSJv1jZd",
    "outputId": "838e326a-a8ff-4ecc-d46e-2a701fca2465"
   },
   "outputs": [],
   "source": [
    "print(list(model.wv.vocab.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183164,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "l0CQiK_31jXl",
    "outputId": "3e28d41b-d5f7-471d-deed-3dc925945b75"
   },
   "outputs": [],
   "source": [
    "print(modelDS.wv['gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182953,
     "status": "ok",
     "timestamp": 1585118907019,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "9Fd6M6sB1jWS",
    "outputId": "9d4f742e-4eb3-46f5-96b3-1585e1d49286"
   },
   "outputs": [],
   "source": [
    "w1 = \"gold\"\n",
    "modelDS.wv.most_similar(positive = w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182749,
     "status": "ok",
     "timestamp": 1585118907020,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "hZajnE271jUr",
    "outputId": "6158b447-428b-4215-8db1-0007a6f42e89"
   },
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"iron\"]\n",
    "modelDS.wv.most_similar(positive = w1, topn = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 182505,
     "status": "ok",
     "timestamp": 1585118907020,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "_EhVlOOI1jRC",
    "outputId": "fb6a8fe3-5a25-4b44-f360-07720a4cd428"
   },
   "outputs": [],
   "source": [
    "# get everything related to stuff on the commodity\n",
    "w1 = [\"gold\",'commodity','ore']\n",
    "w2 = ['rock']\n",
    "modelDS.wv.most_similar(positive = w1, negative = w2, topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181999,
     "status": "ok",
     "timestamp": 1585118907021,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "4TfG_p_r1jOr",
    "outputId": "f644c164-f06e-4e66-b1ee-41f65a25cf4b"
   },
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"ore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181617,
     "status": "ok",
     "timestamp": 1585118907346,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "oaltQ0Eh1jMg",
    "outputId": "b3c66c4c-484d-479a-b266-84d3b95c00ee"
   },
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181462,
     "status": "ok",
     "timestamp": 1585118907347,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "4qNuYbd11jJ9",
    "outputId": "bc326fc8-a38f-496d-afda-24cd6c0ad92d"
   },
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "modelDS.wv.similarity(w1 = \"gold\", w2 = \"rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181337,
     "status": "ok",
     "timestamp": 1585118907348,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "VbUayj321pOz",
    "outputId": "c0f8a95d-07ff-43ab-b444-cfed0901738c"
   },
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "modelDS.wv.doesnt_match([\"gold\", \"rock\", \"copper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIW9Bl-I3KEK"
   },
   "source": [
    "### Comparing our domain-specific embeddings with embeddings trained on a different dataset (news articles).\n",
    "- What are the nearest terms to 'commodity', 'ore', 'rock', etc.\n",
    "\n",
    "Unfortunately due to the size of pretrained word embedding models they have been omitted from this comparison. However, the code below shows how to load the Google news word2vec embedding model. This model is trained on 100 billion words and is 1.6GB in size (massive!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2df1AaGp8-iI"
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "# 1.6GB trained over 100B words; dimension of 300. Cannot use in this notebook due to massive size and time required to load.\n",
    "\n",
    "#import gensim.downloader as api\n",
    "#wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPmtpYf-3KBz"
   },
   "outputs": [],
   "source": [
    "# Define function to compare top-n similariries for a given word between two embedding models.\n",
    "def compare_words(word, topn, model1, model2):\n",
    "    similarWordsModel1 = model1.wv.most_similar(positive=word, topn=topn)\n",
    "    similarWordsModel2 = model2.wv.most_similar(positive=word,topn=topn)\n",
    "\n",
    "    print(f'Top {topn} words similar to {word}\\n(format: n | model 1 | model 2 )\\n')\n",
    "    for n in range(topn):\n",
    "        print(f'{n+1} |{similarWordsModel1[n][0]:<15} | {similarWordsModel2[n][0]:<15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180612,
     "status": "ok",
     "timestamp": 1585118907349,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "gcX31xIr3J9i",
    "outputId": "75a2f13c-b992-4679-bb57-12b6f0867b84"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'commodity'\n",
    "compare_words(word = 'commodity', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180378,
     "status": "ok",
     "timestamp": 1585118907350,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "ExSMep_7RC1S",
    "outputId": "7861088b-996e-4634-e9fb-54cf29c611a8"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'ore'\n",
    "compare_words(word = 'ore', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180154,
     "status": "ok",
     "timestamp": 1585118907350,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "_3b-ykPERCyV",
    "outputId": "075f24e4-2e76-4893-81f2-1734097b95b1"
   },
   "outputs": [],
   "source": [
    "# Looking at the word 'rock'\n",
    "compare_words(word = 'rock', topn = 5, model1 = model, model2 = modelDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUFDEGVK1pMd"
   },
   "source": [
    "### Visualising the domain-specific word vectors in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhE0JjYr1pII"
   },
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,50), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.title(f'Words closest to: {word}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1472,
     "status": "ok",
     "timestamp": 1585118947324,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "qnoeIGsI1pGC",
    "outputId": "b1d2125a-20b0-4bbe-b25b-678e820ec8a0"
   },
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(model = modelDS.wv, word = \"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pzvw14B8TL7A"
   },
   "source": [
    "#### Visualising all of the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1585118951914,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "NiouXHvb1pEB",
    "outputId": "6831cc9a-22c5-474f-8c9d-127582d72f83"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(type(modelDS.wv.vocab))\n",
    "X = modelDS[modelDS.wv.vocab]\n",
    "# Shape of our model before t-SNE\n",
    "print(f'Shape of model before t-SNE: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 341654,
     "status": "ok",
     "timestamp": 1585121616330,
     "user": {
      "displayName": "Tyler Bikaun",
      "photoUrl": "",
      "userId": "18384825377619195869"
     },
     "user_tz": -480
    },
    "id": "tmnytvul10Xf",
    "outputId": "e6fe0879-b67b-4e98-b9dc-e5cd8a49078e"
   },
   "outputs": [],
   "source": [
    "# Fitting subset of data to t-SNE\n",
    "points = 2500\n",
    "X_limited = X[:points]\n",
    "X_tsne = tsne.fit_transform(X_limited)\n",
    "\n",
    "# Uncomment line below to fit entire model to t-SNE\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfCESffv10TD"
   },
   "source": [
    "#### Interactive Visualisation\n",
    "\n",
    "Refer to https://www.datascience.com/resources/notebooks/word-embeddings-in-python, and also for ideas of incoporating POS and bigrams into word2vec training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-YfpTUH10Qq"
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viZhA7RO10N8"
   },
   "outputs": [],
   "source": [
    "def interactive_tsne(text_labels, tsne_array):\n",
    "    '''makes an interactive scatter plot with text labels for each point'''\n",
    "\n",
    "    # Define a dataframe to be used by bokeh context\n",
    "    bokeh_df = pd.DataFrame(tsne_array, text_labels, columns=['x','y'])\n",
    "    bokeh_df['text_labels'] = bokeh_df.index\n",
    "\n",
    "    # interactive controls to include to the plot\n",
    "    TOOLS=\"hover, zoom_in, zoom_out, box_zoom, undo, redo, reset, box_select\"\n",
    "\n",
    "    p = figure(tools=TOOLS, plot_width=700, plot_height=700)\n",
    "\n",
    "    # define data source for the plot\n",
    "    source = ColumnDataSource(bokeh_df)\n",
    "\n",
    "    # scatter plot\n",
    "    p.scatter('x', 'y', source=source, fill_alpha=0.6,\n",
    "              fill_color=\"#8724B5\",\n",
    "              line_color=None)\n",
    "\n",
    "    # text labels\n",
    "    labels = LabelSet(x='x', y='y', text='text_labels', y_offset=8,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # show plot inline\n",
    "    output_notebook()\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch interactive visualisation of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRvKJ6gK10MD"
   },
   "outputs": [],
   "source": [
    "interactive_tsne(list(modelDS.wv.vocab.keys())[:points], X_tsne)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "12.2.2-WordEmbeddings.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
